{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[ 0.8997614  -0.68561006]\n",
       "  [-0.438372    0.10900874]\n",
       "  [-0.24237496  0.0744305 ]\n",
       "  [-0.878746    0.7767273 ]\n",
       "  [ 0.3479402  -0.88589776]\n",
       "  [-1.0920666   0.4524119 ]\n",
       "  [-1.2167361  -1.0090067 ]\n",
       "  [ 0.3935671   1.0544698 ]\n",
       "  [-0.24898548  1.989548  ]\n",
       "  [ 1.199108    1.6460917 ]\n",
       "  [ 0.2609435  -0.9187904 ]\n",
       "  [-0.21924148  1.20272   ]\n",
       "  [ 0.70329595  0.9105568 ]\n",
       "  [-0.38999596 -0.05547183]\n",
       "  [ 0.1282245   2.37668   ]\n",
       "  [-0.19772758  2.0854106 ]\n",
       "  [ 0.5125954  -1.5752442 ]\n",
       "  [ 0.5586102   0.19766884]\n",
       "  [ 2.1481535  -0.19020747]\n",
       "  [ 0.27743885 -0.9118177 ]\n",
       "  [-1.9265597   0.20007446]\n",
       "  [-0.9868158   0.3224166 ]\n",
       "  [-0.1010764   0.02885438]\n",
       "  [-0.05574904 -0.7670688 ]\n",
       "  [-0.14989796 -1.0163515 ]\n",
       "  [ 0.37264898  1.4637082 ]\n",
       "  [ 0.16197115 -0.27533087]\n",
       "  [-0.6607965  -0.0401484 ]\n",
       "  [ 1.5821487  -1.3058236 ]\n",
       "  [-0.8360042  -0.82413524]\n",
       "  [ 1.0865679  -1.4994327 ]\n",
       "  [-1.4236078   0.02837564]\n",
       "  [ 0.8418826  -0.10568757]\n",
       "  [ 0.04687965  0.37848872]\n",
       "  [ 1.2343034  -0.9906856 ]\n",
       "  [ 0.03354292  1.3478173 ]\n",
       "  [ 1.0545628   0.8062987 ]\n",
       "  [-1.4181302   0.5277827 ]\n",
       "  [-0.19834557  1.1042248 ]\n",
       "  [ 1.1090299   2.0841856 ]\n",
       "  [ 0.5614991   0.47623911]\n",
       "  [-1.627182    0.26725844]\n",
       "  [ 1.9553313  -0.7391326 ]\n",
       "  [-0.74201846  0.8339019 ]\n",
       "  [-1.5006033   0.85468733]\n",
       "  [-2.600076    1.2006611 ]\n",
       "  [-0.09968652 -0.4274681 ]\n",
       "  [ 1.534215    0.502874  ]\n",
       "  [-0.5420764  -1.4662498 ]\n",
       "  [-0.8472625  -1.1579773 ]\n",
       "  [-0.600691    0.05431196]\n",
       "  [ 0.22879006  1.0659082 ]\n",
       "  [-1.7202955   0.28747115]\n",
       "  [ 1.9047385   1.5237433 ]\n",
       "  [ 0.9028696   0.05954906]\n",
       "  [-0.1553183  -1.2079413 ]\n",
       "  [ 0.5892751   0.65562326]\n",
       "  [ 0.20619893 -0.17643832]\n",
       "  [-0.05087833 -1.0953387 ]\n",
       "  [-0.5711226  -0.19803372]\n",
       "  [-1.0478936  -0.9163434 ]\n",
       "  [-1.3336002   0.636083  ]\n",
       "  [-0.33219483  1.2063538 ]\n",
       "  [-0.50075674  0.9667549 ]\n",
       "  [-0.46244988  0.27516472]\n",
       "  [ 0.03172834 -2.2662334 ]\n",
       "  [-0.1055946   0.700264  ]\n",
       "  [ 0.30801338 -0.5782221 ]\n",
       "  [-0.20607154  0.693279  ]\n",
       "  [ 0.6269737  -0.35832733]\n",
       "  [-1.3326569  -0.8396715 ]\n",
       "  [-1.2918315  -1.6293093 ]\n",
       "  [-0.7813579  -0.2976086 ]\n",
       "  [-0.01603804  0.6482156 ]\n",
       "  [-1.1087371  -0.06948279]\n",
       "  [-0.6357437   1.1511858 ]\n",
       "  [-0.16127418  0.34598973]\n",
       "  [-0.5449143  -0.55088896]\n",
       "  [-0.53049916 -0.41114143]\n",
       "  [ 0.2638282  -0.6453085 ]\n",
       "  [ 0.68439865 -0.04890326]\n",
       "  [ 0.02328648 -0.16014808]\n",
       "  [-0.7801847   0.12058286]\n",
       "  [ 0.7193251   0.16335344]\n",
       "  [ 0.46283942 -1.5241615 ]\n",
       "  [ 0.0095202   2.3447704 ]\n",
       "  [ 1.4452322   1.6769009 ]\n",
       "  [-0.6372913   0.4833494 ]\n",
       "  [-0.01161444  0.9083601 ]\n",
       "  [ 1.2296662   0.24398364]\n",
       "  [ 1.5268717   1.8360031 ]\n",
       "  [ 0.94724864  0.47048146]\n",
       "  [ 1.4172789   1.0515909 ]\n",
       "  [-0.19438495  0.32892177]\n",
       "  [ 1.8646097  -0.818179  ]\n",
       "  [-0.06971561 -0.68682677]\n",
       "  [-1.3441135  -0.4652605 ]\n",
       "  [ 0.26944658  0.45839924]\n",
       "  [ 1.1732304  -0.01763551]\n",
       "  [-1.3559734   1.1798365 ]]\n",
       " <NDArray 100x2 @cpu(0)>, \n",
       " [ 2.3664443   0.6524414   1.1321378   0.43179327  1.4517586  -0.26468825\n",
       "  -0.8370806   1.978476    1.5765183   3.6033752   1.2969532   1.273926\n",
       "   2.625861    0.7296446   2.3237424   1.7832855   1.2608972   2.1888018\n",
       "   4.306071    1.3268889  -1.5716257  -0.09305978  1.1260184   0.87098175\n",
       "   0.5993483   2.2404492   1.1935339   0.15046698  2.7222376  -0.25399342\n",
       "   2.1041625  -0.8132422   2.4978878   1.4216932   2.6210039   1.9152999\n",
       "   2.9601018  -0.4659102   1.2565897   3.4970975   2.361012   -1.0334468\n",
       "   3.467377    0.4876202  -0.6898425  -1.9591361   0.9355608   3.5365052\n",
       "  -0.24489123 -0.5904499   0.30647504  2.0740952  -0.99455786  4.3687162\n",
       "   2.3570666   0.5632745   2.47182     1.4274583   0.49943945  0.44755542\n",
       "  -0.7438419  -0.41647214  1.1247495   0.77162427  0.41482055  0.12299867\n",
       "   1.2437396   1.526907    1.3938854   1.9577256  -1.0267977  -1.2205404\n",
       "   0.08730251  1.4527689  -0.38384265  0.90459615  1.099421    0.33046123\n",
       "   0.36092648  1.173634    2.1197708   1.1051242   0.07314415  2.419408\n",
       "   1.2172953   2.1151652   3.8052197   0.53343964  1.3998888   2.9920168\n",
       "   4.2049675   2.6862595   3.6740706   0.9227755   3.5660322   0.97155434\n",
       "  -0.9350863   1.9582608   2.6886747  -0.1806684 ]\n",
       " <NDArray 100 @cpu(0)>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mxnet.ndarray as nd\n",
    "import mxnet.autograd as ag\n",
    "features = 2\n",
    "examples = 100\n",
    "X = nd.random_normal(shape=(examples, features))\n",
    "true_w = [1.4, 0.4]\n",
    "true_b = 1.2\n",
    "Y = true_w[0]*X[:, 0]+true_w[1]*X[:, 1]+true_b\n",
    "Y += 0.1*nd.random_normal(shape=Y.shape)\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 1.2296662   0.24398364]\n",
      " [-1.2167361  -1.0090067 ]\n",
      " [ 0.3935671   1.0544698 ]\n",
      " [-1.0920666   0.4524119 ]\n",
      " [-0.6607965  -0.0401484 ]\n",
      " [ 1.8646097  -0.818179  ]\n",
      " [ 0.68439865 -0.04890326]\n",
      " [-0.9868158   0.3224166 ]\n",
      " [ 0.37264898  1.4637082 ]\n",
      " [-0.33219483  1.2063538 ]]\n",
      "<NDArray 10x2 @cpu(0)> \n",
      "[ 2.9920168  -0.8370806   1.978476   -0.26468825  0.15046698  3.5660322\n",
      "  2.1197708  -0.09305978  2.2404492   1.1247495 ]\n",
      "<NDArray 10 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import gluon as gl \n",
    "batch_size=10 \n",
    "dataset=gl.data.ArrayDataset(X,Y)\n",
    "data_iter=gl.data.DataLoader(dataset,batch_size,shuffle=True)\n",
    "for X,y in data_iter:\n",
    "    print(X,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "net=nn.Sequential()\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize()\n",
    "from mxnet.gluon import loss as gloss\n",
    "loss=gloss.L2Loss();\n",
    "from mxnet import gluon\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':.02 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 ,avg loss: 1.522305\n",
      "epoch: 2 ,avg loss: 1.033264\n",
      "epoch: 3 ,avg loss: 0.701900\n",
      "epoch: 4 ,avg loss: 0.479606\n",
      "epoch: 5 ,avg loss: 0.329168\n",
      "epoch: 6 ,avg loss: 0.227403\n",
      "epoch: 7 ,avg loss: 0.157927\n",
      "epoch: 8 ,avg loss: 0.110691\n",
      "epoch: 9 ,avg loss: 0.078360\n",
      "epoch: 10 ,avg loss: 0.055886\n",
      "epoch: 11 ,avg loss: 0.040773\n",
      "epoch: 12 ,avg loss: 0.030088\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd\n",
    "epochs = 12\n",
    "for e in range(1, epochs+1):\n",
    "    total_loss = 0\n",
    "    for X, y in data_iter:\n",
    "        with autograd.record():\n",
    "            output = net(X)\n",
    "            los = loss(output, y)\n",
    "        los.backward()\n",
    "        trainer.step(batch_size)\n",
    "        total_loss += nd.sum(los).asscalar()\n",
    "    print('epoch: %d ,avg loss: %f' % (e, total_loss/examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.4, 0.4], \n",
       " [[1.232498   0.41518816]]\n",
       " <NDArray 1x2 @cpu(0)>, 1.2, \n",
       " [1.0903422]\n",
       " <NDArray 1 @cpu(0)>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense=net[0]\n",
    "true_w,dense.weight.data(),true_b,dense.bias.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method step in module mxnet.gluon.trainer:\n",
      "\n",
      "step(batch_size, ignore_stale_grad=False) method of mxnet.gluon.trainer.Trainer instance\n",
      "    Makes one step of parameter update. Should be called after\n",
      "    `autograd.backward()` and outside of `record()` scope.\n",
      "    \n",
      "    For normal parameter updates, `step()` should be used, which internally calls\n",
      "    `allreduce_grads()` and then `update()`. However, if you need to get the reduced\n",
      "    gradients to perform certain transformation, such as in gradient clipping, then\n",
      "    you may want to manually call `allreduce_grads()` and `update()` separately.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    batch_size : int\n",
      "        Batch size of data processed. Gradient will be normalized by `1/batch_size`.\n",
      "        Set this to 1 if you normalized loss manually with `loss = mean(loss)`.\n",
      "    ignore_stale_grad : bool, optional, default=False\n",
      "        If true, ignores Parameters with stale gradient (gradient that has not\n",
      "        been updated by `backward` after last step) and skip update.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(trainer.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "&**\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit976359a467c3400289726740d00d4381"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
