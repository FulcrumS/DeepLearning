{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import nd\n",
    "from mxnet import autograd\n",
    "def transform(data,label):\n",
    "    return data.astype('float32')/255,label.astype('float32')\n",
    "minist_train=gluon.data.vision.FashionMNIST(root='fashion-mnist/',train=True,transform=transform)\n",
    "minist_test=gluon.data.vision.FashionMNIST(root='fashion-mnist/',train=False,transform=transform)\n",
    "batch_size=256 \n",
    "train_data=gluon.data.DataLoader(minist_train,batch_size=batch_size,shuffle=True)\n",
    "test_data=gluon.data.DataLoader(minist_test,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 两层隐含层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "net=gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(256,activation='relu'))\n",
    "    net.add(gluon.nn.Dense(10))\n",
    "net.initialize()\n",
    "softmax_crossEntropy=gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output,label):\n",
    "    return nd.mean(output.argmax(axis=1)==label).asscalar()\n",
    "def evaluate_accuracy(test_data,net):\n",
    "    acc=0. \n",
    "    for data,label in test_data:\n",
    "        output=net(data)\n",
    "        acc+=accuracy(output,label)\n",
    "    return acc/len(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1,loss:0.722239..,train_accuracy:0.737295,test_accuracy:0.020984..\n",
      "epoch 2,loss:0.467453..,train_accuracy:0.827521,test_accuracy:0.021455..\n",
      "epoch 3,loss:0.410668..,train_accuracy:0.850421,test_accuracy:0.021707..\n",
      "epoch 4,loss:0.378833..,train_accuracy:0.860029,test_accuracy:0.021685..\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd\n",
    "epochs = 4\n",
    "for e in range(1, epochs+1):\n",
    "    train_acc, train_loss = 0., 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_crossEntropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        train_acc += accuracy(output, label)\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "    test_acc = evaluate_accuracy(test_data, net)\n",
    "    print('epoch %d,loss:%f..,train_accuracy:%f,test_accuracy:%f..' % (e, train_loss/len(train_data), train_acc/len(train_data),\n",
    "                                                                       test_acc/len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " [[ 0.00679919  0.01292737  0.03033251 ... -0.02541603 -0.0150122\n",
       "    0.05347459]\n",
       "  [ 0.06538177  0.05608409  0.00654458 ... -0.01343163  0.02962739\n",
       "    0.04782937]\n",
       "  [ 0.05228012 -0.06844398  0.06559557 ... -0.05495931  0.00956061\n",
       "   -0.04069941]\n",
       "  ...\n",
       "  [-0.06626982 -0.0615694   0.04073513 ...  0.02512913 -0.0019495\n",
       "    0.01449291]\n",
       "  [ 0.04740904  0.01716206  0.02910219 ...  0.03891923  0.06823969\n",
       "    0.03291409]\n",
       "  [ 0.03623506  0.01020497 -0.04744481 ... -0.03525304 -0.01251852\n",
       "   -0.06929076]]\n",
       " <NDArray 256x784 @cpu(0)>, \n",
       " [ 2.16267973e-01  2.10423648e-01 -3.91482096e-03  1.48102388e-01\n",
       "   5.17111719e-02  3.33081037e-02 -3.59931886e-02  2.29611881e-02\n",
       "   1.06728747e-01  9.28028896e-02  2.96972245e-01  9.33107436e-02\n",
       "   2.91183833e-02  4.90503348e-02  1.36192217e-01  1.85357884e-01\n",
       "   9.90470685e-03 -3.22347833e-03  4.41323640e-03 -2.76640933e-02\n",
       "   1.60672572e-02  3.04576885e-02  1.47342468e-02  1.05687585e-02\n",
       "   1.37642741e-01 -9.56155267e-03 -1.32239079e-02 -1.24927564e-02\n",
       "   1.12767279e-01  1.12819284e-01 -2.20621731e-02  9.77178290e-02\n",
       "  -2.08684914e-02  1.88268125e-02 -1.79962087e-02 -6.98850527e-02\n",
       "   1.39079675e-01 -4.47052578e-03  6.82774708e-02  1.46156251e-02\n",
       "  -3.10699847e-02  1.04126029e-01  4.78901491e-02  9.35385078e-02\n",
       "  -1.38304094e-02  2.91861624e-01 -1.75066702e-02  1.82117686e-01\n",
       "   1.63930804e-01 -2.98737502e-03 -1.30011871e-01 -3.86278592e-02\n",
       "   8.61587301e-02 -1.49218468e-02  1.00635372e-01  7.77807832e-02\n",
       "   1.60290405e-01 -1.20442361e-01  1.06011987e-01  4.70057353e-02\n",
       "   3.85103136e-01  7.00990576e-03  3.52840796e-02  1.76613051e-02\n",
       "  -1.06458087e-02  2.65508384e-01 -1.59326307e-02  4.82876152e-02\n",
       "   5.66364080e-02  2.02752680e-01  2.07665816e-01  3.11928391e-02\n",
       "   6.32889057e-03  1.34287044e-01  1.25254676e-01  2.05908209e-01\n",
       "  -8.06644261e-02  7.84202889e-02  1.70216531e-01  2.01013103e-01\n",
       "  -9.39411856e-03 -6.15691394e-02  3.74015048e-03 -4.53244783e-02\n",
       "  -1.19135998e-01  8.63956735e-02 -1.00484274e-01  4.75454703e-02\n",
       "   7.99723491e-02 -6.77903090e-03 -1.41195506e-01  3.61090861e-02\n",
       "   9.25449878e-02  1.37457862e-01  4.10004258e-02  7.61043578e-02\n",
       "   3.91258448e-02  2.12484971e-02  6.42117253e-03  2.79505607e-02\n",
       "   3.36553268e-02 -5.96495196e-02  5.48971929e-02 -1.60776973e-02\n",
       "  -1.76386043e-01  1.21761315e-01  2.68699914e-01 -8.97400379e-02\n",
       "   7.62185678e-02  6.05410077e-02  6.36284146e-03  4.69645485e-02\n",
       "  -8.94738287e-02  1.04836039e-01  1.26944408e-01 -6.98442431e-03\n",
       "  -4.49558208e-03  1.36817232e-01 -3.52251753e-02  1.15755247e-03\n",
       "   1.05647855e-01  1.05912052e-01 -8.36606510e-03 -2.85895430e-02\n",
       "   1.02564923e-01  1.07110061e-01 -2.42741825e-03  3.28729779e-01\n",
       "   6.27015308e-02  5.77011099e-03  7.63439015e-02  1.06759124e-01\n",
       "  -5.87822720e-02  1.07236043e-01 -2.25988217e-02  1.82540670e-01\n",
       "  -1.89190153e-02 -2.23472547e-02  1.00151092e-01 -6.76578423e-03\n",
       "  -5.29422006e-03  1.30752604e-02  5.32999747e-02  1.89772174e-02\n",
       "   1.36344597e-01  7.84788281e-02 -1.05449697e-02 -6.61206692e-02\n",
       "   6.29967526e-02  4.64283451e-02  9.53049213e-03  1.55587584e-01\n",
       "   7.31702670e-02 -3.17416489e-02  9.33338404e-02  3.83933187e-01\n",
       "  -9.71481204e-03 -5.12426603e-04  2.90272236e-01 -1.12068690e-02\n",
       "   1.23046987e-01  4.76198830e-02  1.34906873e-01  1.35863692e-01\n",
       "   6.09393828e-02 -1.45561630e-02  1.25090510e-01  1.17490858e-01\n",
       "  -7.16863386e-03 -2.07755882e-02  1.28297687e-01  7.56519064e-02\n",
       "  -3.34397517e-02  3.07048321e-01 -3.73248570e-02  1.14211649e-01\n",
       "   1.77191183e-01 -1.39146775e-01 -2.91736983e-03  3.01750928e-01\n",
       "   5.35887815e-02  2.69213580e-02  6.99856430e-02  5.53643629e-02\n",
       "   1.49368241e-01 -1.92212183e-02  1.54660016e-01  1.34249870e-02\n",
       "   2.69902021e-01  2.13160533e-02 -3.16508263e-02  1.88574165e-01\n",
       "   1.07886881e-01  1.25076726e-01  5.25316596e-02  3.08256179e-01\n",
       "   1.63650900e-01  2.73814276e-02  1.25684902e-01  1.39114439e-01\n",
       "  -2.19892673e-02  1.18345581e-01 -3.91878141e-03 -3.77422571e-03\n",
       "  -2.60760505e-02  2.10799538e-02 -1.29076811e-02  5.69916181e-02\n",
       "   9.58008245e-02  8.21919292e-02  2.10221007e-01  1.14350428e-03\n",
       "   7.63114616e-02  2.46683005e-02  9.74244252e-02  2.65896380e-01\n",
       "  -4.22721641e-05  1.14280030e-01  7.19665587e-02 -7.82126933e-03\n",
       "   8.69629160e-02  1.23480245e-01  3.48310582e-02 -2.50608977e-02\n",
       "  -2.55540721e-02  4.85409796e-02 -3.13158371e-02  1.02515809e-01\n",
       "  -2.63059288e-02 -6.46752268e-02  1.19973056e-01 -4.85722795e-02\n",
       "   1.17063105e-01 -2.24955350e-01  3.21596372e-03 -1.47096924e-02\n",
       "   8.39558914e-02  2.10429892e-01  9.50910524e-02  2.32301634e-02\n",
       "  -8.89629945e-02  3.39838937e-02  4.80827205e-02 -6.11804752e-03\n",
       "   1.04136273e-01  1.24446556e-01  2.10875705e-01  1.49177596e-01\n",
       "   5.22581153e-02 -8.69002417e-02 -1.12063950e-02 -9.42273997e-03\n",
       "  -1.10204546e-02  1.20594008e-02 -1.93223683e-03  1.23648439e-02]\n",
       " <NDArray 256 @cpu(0)>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense=net[1]\n",
    "dense.weight.data(),dense.bias.data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit976359a467c3400289726740d00d4381"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
