{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为强化学习很早期也很经典的一种算法，Q-Learning是一种基于值(Value Based)的算法。至于为什么叫 Q-Learning，是因为其本身是一种依靠 Q 函数来寻找最优的动作-状态决策的。关于 Q-Learning算法的细节和原理笔者这里不做详细描述，感兴趣的朋友可以直接研读相关论文。\n",
    "> Watkins C J C H, Dayan P. Technical Note: Q-Learning[J]. Machine Learning, 1992, 8(3-4):279-292."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning算法描述：**\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601103840585.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0lfYW1fYV9idWdlcg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关模块并 预设参数值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T02:52:05.677564Z",
     "start_time": "2020-06-01T02:52:05.667091Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "# 状态宽度\n",
    "N_STATES = 6   \n",
    "# 探索者的可用动作\n",
    "\n",
    "ACTIONS = ['left', 'right']\n",
    "# 贪心率\n",
    "EPSILON = 0.9   \n",
    "# 学习率\n",
    "ALPHA = 0.1   \n",
    "# 奖励递减值\n",
    "GAMMA = 0.9   \n",
    "# 最大回合数\n",
    "MAX_EPISODES = 13  \n",
    "# 移动间隔时间\n",
    "FRESH_TIME = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 Q-Table："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T02:52:05.693562Z",
     "start_time": "2020-06-01T02:52:05.677564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   left  right\n",
       "0   0.0    0.0\n",
       "1   0.0    0.0\n",
       "2   0.0    0.0\n",
       "3   0.0    0.0\n",
       "4   0.0    0.0\n",
       "5   0.0    0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_table(n_states,actions):\n",
    "    q_table=pd.DataFrame(np.zeros((n_states,len(actions))),columns=actions)\n",
    "    return q_table\n",
    "q_table=build_table(N_STATES,ACTIONS)\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义在某个状态地点State，选择行为Action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T02:52:05.701563Z",
     "start_time": "2020-06-01T02:52:05.693562Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def choose_action(S,q_table):\n",
    "    # 选出这个 state 的所有 action 值\n",
    "    action_choice=q_table.iloc[S,:]\n",
    "    # 非贪婪 or 或者这个 state 还没有探索过\n",
    "    if np.random.rand()>EPSILON or action_choice.all()==0:\n",
    "        action=np.random.choice(ACTIONS)\n",
    "    else:\n",
    "        # 贪心策略\n",
    "        action=ACTIONS[action_choice.argmax()]\n",
    "    return action\n",
    "choose_action(3,q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义环境反馈过程，以奖励R的形式给出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T02:52:05.765588Z",
     "start_time": "2020-06-01T02:52:05.757596Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_env_feedback(S,A):\n",
    "    # 主体与环境的交互过程\n",
    "    if A=='right':\n",
    "#         到达最右端是最好的，奖励值也是最大的\n",
    "        if S==N_STATES-2:\n",
    "            S_='terminal'\n",
    "            R=5\n",
    "        else:\n",
    "#             下一步是往右走，接近目标，奖励一下\n",
    "            S_=S+1 \n",
    "            R=1 \n",
    "    else:\n",
    "#         目标是最右端，此时反而往左走，不奖励\n",
    "        S_=S if S==0 else S-1 \n",
    "        R=0 \n",
    "    return S_,R "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 更新环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T02:52:05.773590Z",
     "start_time": "2020-06-01T02:52:05.765588Z"
    }
   },
   "outputs": [],
   "source": [
    "def update_env(S,episode,step_counter):\n",
    "    env_list=['-']*(N_STATES-1)+['T']\n",
    "    if S=='terminal':\n",
    "        print(f\"\\rEpisode:{episode},total_step:{step_counter}\")\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        env_list[S]='o'\n",
    "        print(f\"\\r{''.join(env_list)}\",end=\"\")\n",
    "        time.sleep(FRESH_TIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 Q-Learning训练过程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Learning算法描述：**\n",
    "![在这里插入图片描述](https://img-blog.csdnimg.cn/20200601103840585.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0lfYW1fYV9idWdlcg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T02:52:05.781585Z",
     "start_time": "2020-06-01T02:52:05.773590Z"
    }
   },
   "outputs": [],
   "source": [
    "def q_train():\n",
    "    q_table=build_table(N_STATES,ACTIONS)\n",
    "    for episode in range(1,MAX_EPISODES+1):\n",
    "        '''\n",
    "        S: 回合初始位置\n",
    "        step_counter：此回合走的步数\n",
    "        is_terminal：是否回合结束\n",
    "        '''\n",
    "        S,step_counter,is_terminal=0,0,False \n",
    "        update_env(S,episode,step_counter)\n",
    "        while not is_terminal:\n",
    "            A=choose_action(S,q_table)  # 选行为\n",
    "            S_,R=get_env_feedback(S,A)  # 实施行为并得到环境的反馈\n",
    "            q_predict=q_table.loc[S,A] # 估算的(状态-行为)值\n",
    "            if S_=='terminal':\n",
    "                q_target=R \n",
    "                # 实际的(状态-行为)值 (回合结束)\n",
    "                is_terminal=True \n",
    "            else:\n",
    "                 # 实际的(状态-行为)值 (回合没结束)\n",
    "                q_target=R+GAMMA*q_table.iloc[S_,:].max()\n",
    "             # 更新 q_table\n",
    "            q_table.loc[S,A]+=ALPHA*(q_target-q_predict)\n",
    "            S=S_ \n",
    "            step_counter+=1\n",
    "               # 环境更新\n",
    "            update_env(S,episode,step_counter)\n",
    "    return q_table\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-01T02:53:04.675520Z",
     "start_time": "2020-06-01T02:52:05.781585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1,total_step:32\n",
      "Episode:2,total_step:5\n",
      "Episode:3,total_step:7\n",
      "Episode:4,total_step:11\n",
      "Episode:5,total_step:5\n",
      "Episode:6,total_step:5\n",
      "Episode:7,total_step:9\n",
      "Episode:8,total_step:5\n",
      "Episode:9,total_step:8\n",
      "Episode:10,total_step:7\n",
      "Episode:11,total_step:5\n",
      "Episode:12,total_step:5\n",
      "Episode:13,total_step:5\n",
      "\n",
      "Q_table:\n",
      "       left     right\n",
      "0  0.112040  1.686950\n",
      "1  0.234920  1.737197\n",
      "2  0.180106  1.991320\n",
      "3  0.303024  2.577075\n",
      "4  0.111257  3.729067\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "q_table=q_train()\n",
    "print(\"\\r\\nQ_table:\")\n",
    "print(q_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit",
   "language": "python",
   "name": "python37564bita5f53f9b60c0490aa934da503dc99db2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
